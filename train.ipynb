{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcW5UMMGVIyt"
      },
      "source": [
        "#!/usr/bin/env python3 -u\n",
        "# Copyright (c) 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the LICENSE file in\n",
        "# the root directory of this source tree.\n",
        "from __future__ import print_function\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable, grad\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils import random_perturb, make_step, inf_data_gen, Logger\n",
        "from utils import soft_cross_entropy, classwise_loss, LDAMLoss, FocalLoss\n",
        "from config import *\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "LOGNAME = 'Imbalance_' + LOGFILE_BASE\n",
        "logger = Logger(LOGNAME)\n",
        "LOGDIR = logger.logdir\n",
        "\n",
        "LOG_CSV = os.path.join(LOGDIR, f'log_{SEED}.csv')\n",
        "LOG_CSV_HEADER = [\n",
        "    'epoch', 'train loss', 'gen loss', 'train acc', 'gen_acc', 'prob_orig', 'prob_targ',\n",
        "    'test loss', 'major test acc', 'neutral test acc', 'minor test acc', 'test acc', 'f1 score'\n",
        "]\n",
        "if not os.path.exists(LOG_CSV):\n",
        "    with open(LOG_CSV, 'w') as f:\n",
        "        csv_writer = csv.writer(f, delimiter=',')\n",
        "        csv_writer.writerow(LOG_CSV_HEADER)\n",
        "\n",
        "\n",
        "def save_checkpoint(acc, model, optim, epoch, index=False):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model = model.module\n",
        "\n",
        "    state = {\n",
        "        'net': model.state_dict(),\n",
        "        'optimizer': optim.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state()\n",
        "    }\n",
        "\n",
        "    if index:\n",
        "        ckpt_name = 'ckpt_epoch' + str(epoch) + '_' + str(SEED) + '.t7'\n",
        "    else:\n",
        "        ckpt_name = 'ckpt_' + str(SEED) + '.t7'\n",
        "\n",
        "    ckpt_path = os.path.join(LOGDIR, ckpt_name)\n",
        "    torch.save(state, ckpt_path)\n",
        "\n",
        "\n",
        "def train_epoch(model, criterion, optimizer, data_loader, logger=None):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in tqdm(data_loader):\n",
        "        # For SMOTE, get the samples from smote_loader instead of usual loader\n",
        "        if epoch >= ARGS.warm and ARGS.smote:\n",
        "            inputs, targets = next(smote_loader_inf)\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        batch_size = inputs.size(0)\n",
        "\n",
        "        outputs, _ = model(normalizer(inputs))\n",
        "        loss = criterion(outputs, targets).mean()\n",
        "\n",
        "        train_loss += loss.item() * batch_size\n",
        "        predicted = outputs.max(1)[1]\n",
        "        total += batch_size\n",
        "        correct += sum_t(predicted.eq(targets))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    msg = 'Loss: %.3f| Acc: %.3f%% (%d/%d)' % \\\n",
        "          (train_loss / total, 100. * correct / total, correct, total)\n",
        "    if logger:\n",
        "        logger.log(msg)\n",
        "    else:\n",
        "        print(msg)\n",
        "\n",
        "    return train_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "def uniform_loss(outputs):\n",
        "    weights = torch.ones_like(outputs) / N_CLASSES\n",
        "\n",
        "    return soft_cross_entropy(outputs, weights, reduction='mean')\n",
        "\n",
        "\n",
        "def classwise_loss(outputs, targets):\n",
        "    out_1hot = torch.zeros_like(outputs)\n",
        "    out_1hot.scatter_(1, targets.view(-1, 1), 1)\n",
        "    return (outputs * out_1hot).sum(1).mean()\n",
        "\n",
        "\n",
        "def generation(model_g, model_r, inputs, seed_targets, targets, p_accept,\n",
        "               gamma, lam, step_size, random_start=True, max_iter=10):\n",
        "    model_g.eval()\n",
        "    model_r.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if random_start:\n",
        "        random_noise = random_perturb(inputs, 'l2', 0.5)\n",
        "        inputs = torch.clamp(inputs + random_noise, 0, 1)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        inputs = inputs.clone().detach().requires_grad_(True)\n",
        "        outputs_g, _ = model_g(normalizer(inputs))\n",
        "        outputs_r, _ = model_r(normalizer(inputs))\n",
        "\n",
        "        loss = criterion(outputs_g, targets) + lam * classwise_loss(outputs_r, seed_targets)\n",
        "        grad, = torch.autograd.grad(loss, [inputs])\n",
        "\n",
        "        inputs = inputs - make_step(grad, 'l2', step_size)\n",
        "        inputs = torch.clamp(inputs, 0, 1)\n",
        "\n",
        "    inputs = inputs.detach()\n",
        "\n",
        "    outputs_g, _ = model_g(normalizer(inputs))\n",
        "\n",
        "    one_hot = torch.zeros_like(outputs_g)\n",
        "    one_hot.scatter_(1, targets.view(-1, 1), 1)\n",
        "    probs_g = torch.softmax(outputs_g, dim=1)[one_hot.to(torch.bool)]\n",
        "\n",
        "    correct = (probs_g >= gamma) * torch.bernoulli(p_accept).byte().to(device)\n",
        "    model_r.train()\n",
        "\n",
        "    return inputs, correct\n",
        "\n",
        "\n",
        "def train_net(model_train, model_gen, criterion, optimizer_train, inputs_orig, targets_orig, gen_idx, gen_targets):\n",
        "    batch_size = inputs_orig.size(0)\n",
        "\n",
        "    inputs = inputs_orig.clone()\n",
        "    targets = targets_orig.clone()\n",
        "\n",
        "    ########################\n",
        "\n",
        "    bs = N_SAMPLES_PER_CLASS_T[targets_orig].repeat(gen_idx.size(0), 1)\n",
        "    gs = N_SAMPLES_PER_CLASS_T[gen_targets].view(-1, 1)\n",
        "\n",
        "    delta = F.relu(bs - gs)\n",
        "    p_accept = 1 - ARGS.beta ** delta\n",
        "    mask_valid = (p_accept.sum(1) > 0)\n",
        "\n",
        "    gen_idx = gen_idx[mask_valid]\n",
        "    gen_targets = gen_targets[mask_valid]\n",
        "    p_accept = p_accept[mask_valid]\n",
        "\n",
        "    select_idx = torch.multinomial(p_accept, 1, replacement=True).view(-1)\n",
        "    p_accept = p_accept.gather(1, select_idx.view(-1, 1)).view(-1)\n",
        "\n",
        "    seed_targets = targets_orig[select_idx]\n",
        "    seed_images = inputs_orig[select_idx]\n",
        "\n",
        "    gen_inputs, correct_mask = generation(model_gen, model_train, seed_images, seed_targets, gen_targets, p_accept,\n",
        "                                          ARGS.gamma, ARGS.lam, ARGS.step_size, True, ARGS.attack_iter)\n",
        "\n",
        "    ########################\n",
        "\n",
        "    # Only change the correctly generated samples\n",
        "    num_gen = sum_t(correct_mask)\n",
        "    num_others = batch_size - num_gen\n",
        "\n",
        "    gen_c_idx = gen_idx[correct_mask]\n",
        "    others_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "    others_mask[gen_c_idx] = 0\n",
        "    others_idx = others_mask.nonzero().view(-1)\n",
        "\n",
        "    if num_gen > 0:\n",
        "        gen_inputs_c = gen_inputs[correct_mask]\n",
        "        gen_targets_c = gen_targets[correct_mask]\n",
        "\n",
        "        inputs[gen_c_idx] = gen_inputs_c\n",
        "        targets[gen_c_idx] = gen_targets_c\n",
        "\n",
        "    outputs, _ = model_train(normalizer(inputs))\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    optimizer_train.zero_grad()\n",
        "    loss.mean().backward()\n",
        "    optimizer_train.step()\n",
        "\n",
        "    # For logging the training\n",
        "\n",
        "    oth_loss_total = sum_t(loss[others_idx])\n",
        "    gen_loss_total = sum_t(loss[gen_c_idx])\n",
        "\n",
        "    _, predicted = torch.max(outputs[others_idx].data, 1)\n",
        "    num_correct_oth = sum_t(predicted.eq(targets[others_idx]))\n",
        "\n",
        "    num_correct_gen, p_g_orig, p_g_targ = 0, 0, 0\n",
        "    success = torch.zeros(N_CLASSES, 2)\n",
        "\n",
        "    if num_gen > 0:\n",
        "        _, predicted_gen = torch.max(outputs[gen_c_idx].data, 1)\n",
        "        num_correct_gen = sum_t(predicted_gen.eq(targets[gen_c_idx]))\n",
        "        probs = torch.softmax(outputs[gen_c_idx], 1).data\n",
        "\n",
        "        p_g_orig = probs.gather(1, seed_targets[correct_mask].view(-1, 1))\n",
        "        p_g_orig = sum_t(p_g_orig)\n",
        "\n",
        "        p_g_targ = probs.gather(1, gen_targets_c.view(-1, 1))\n",
        "        p_g_targ = sum_t(p_g_targ)\n",
        "\n",
        "    for i in range(N_CLASSES):\n",
        "        if num_gen > 0:\n",
        "            success[i, 0] = sum_t(gen_targets_c == i)\n",
        "        success[i, 1] = sum_t(gen_targets == i)\n",
        "\n",
        "    return oth_loss_total, gen_loss_total, num_others, num_correct_oth, num_gen, num_correct_gen, p_g_orig, p_g_targ, success\n",
        "\n",
        "\n",
        "def train_gen_epoch(net_t, net_g, criterion, optimizer, data_loader):\n",
        "    net_t.train()\n",
        "    net_g.eval()\n",
        "\n",
        "    oth_loss, gen_loss = 0, 0\n",
        "    correct_oth = 0\n",
        "    correct_gen = 0\n",
        "    total_oth, total_gen = 1e-6, 1e-6\n",
        "    p_g_orig, p_g_targ = 0, 0\n",
        "    t_success = torch.zeros(N_CLASSES, 2)\n",
        "\n",
        "    for inputs, targets in tqdm(data_loader):\n",
        "        batch_size = inputs.size(0)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Set a generation target for current batch with re-sampling\n",
        "        if ARGS.imb_type != 'none':  # Imbalanced\n",
        "            # Keep the sample with this probability\n",
        "            gen_probs = N_SAMPLES_PER_CLASS_T[targets] / N_SAMPLES_PER_CLASS_T[0]\n",
        "            gen_index = (1 - torch.bernoulli(gen_probs)).nonzero()    # Generation index\n",
        "            gen_index = gen_index.view(-1)\n",
        "            gen_targets = targets[gen_index]\n",
        "        else:   # Balanced\n",
        "            gen_index = torch.arange(batch_size).view(-1)\n",
        "            gen_targets = torch.randint(N_CLASSES, (batch_size,)).to(device).long()\n",
        "\n",
        "        t_loss, g_loss, num_others, num_correct, num_gen, num_gen_correct, p_g_orig_batch, p_g_targ_batch, success \\\n",
        "            = train_net(net_t, net_g, criterion, optimizer, inputs, targets, gen_index, gen_targets)\n",
        "\n",
        "        oth_loss += t_loss\n",
        "        gen_loss += g_loss\n",
        "        total_oth += num_others\n",
        "        correct_oth += num_correct\n",
        "        total_gen += num_gen\n",
        "        correct_gen += num_gen_correct\n",
        "        p_g_orig += p_g_orig_batch\n",
        "        p_g_targ += p_g_targ_batch\n",
        "        t_success += success\n",
        "\n",
        "    res = {\n",
        "        'train_loss': oth_loss / total_oth,\n",
        "        'gen_loss': gen_loss / total_gen,\n",
        "        'train_acc': 100. * correct_oth / total_oth,\n",
        "        'gen_acc': 100. * correct_gen / total_gen,\n",
        "        'p_g_orig': p_g_orig / total_gen,\n",
        "        'p_g_targ': p_g_targ / total_gen,\n",
        "        't_success': t_success\n",
        "    }\n",
        "\n",
        "    msg = 't_Loss: %.3f | g_Loss: %.3f | Acc: %.3f%% (%d/%d) | Acc_gen: %.3f%% (%d/%d) ' \\\n",
        "          '| Prob_orig: %.3f | Prob_targ: %.3f' % (\n",
        "        res['train_loss'], res['gen_loss'],\n",
        "        res['train_acc'], correct_oth, total_oth,\n",
        "        res['gen_acc'], correct_gen, total_gen,\n",
        "        res['p_g_orig'], res['p_g_targ']\n",
        "    )\n",
        "    if logger:\n",
        "        logger.log(msg)\n",
        "    else:\n",
        "        print(msg)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TEST_ACC = 0  # best test accuracy\n",
        "    BEST_VAL = 0  # best validation accuracy\n",
        "\n",
        "    # Weights for virtual samples are generated\n",
        "    logger.log('==> Building model: %s' % MODEL)\n",
        "    net = models.__dict__[MODEL](N_CLASSES)\n",
        "    net_seed = models.__dict__[MODEL](N_CLASSES)\n",
        "\n",
        "    net, net_seed = net.to(device), net_seed.to(device)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=ARGS.lr, momentum=0.9, weight_decay=ARGS.decay)\n",
        "\n",
        "    if ARGS.resume:\n",
        "        # Load checkpoint.\n",
        "        logger.log('==> Resuming from checkpoint..')\n",
        "        ckpt_g = f'./checkpoint/{DATASET}/ratio{ARGS.ratio}/erm_trial1_{MODEL}.t7'\n",
        "\n",
        "        if ARGS.net_both is not None:\n",
        "            ckpt_t = torch.load(ARGS.net_both)\n",
        "            net.load_state_dict(ckpt_t['net'])\n",
        "            optimizer.load_state_dict(ckpt_t['optimizer'])\n",
        "            START_EPOCH = ckpt_t['epoch'] + 1\n",
        "            net_seed.load_state_dict(ckpt_t['net2'])\n",
        "        else:\n",
        "            if ARGS.net_t is not None:\n",
        "                ckpt_t = torch.load(ARGS.net_t)\n",
        "                net.load_state_dict(ckpt_t['net'])\n",
        "                optimizer.load_state_dict(ckpt_t['optimizer'])\n",
        "                START_EPOCH = ckpt_t['epoch'] + 1\n",
        "\n",
        "            if ARGS.net_g is not None:\n",
        "                ckpt_g = ARGS.net_g\n",
        "                print(ckpt_g)\n",
        "                ckpt_g = torch.load(ckpt_g)\n",
        "                net_seed.load_state_dict(ckpt_g['net'])\n",
        "\n",
        "    if N_GPUS > 1:\n",
        "        logger.log('Multi-GPU mode: using %d GPUs for training.' % N_GPUS)\n",
        "        net = nn.DataParallel(net)\n",
        "        net_seed = nn.DataParallel(net_seed)\n",
        "    elif N_GPUS == 1:\n",
        "        logger.log('Single-GPU mode.')\n",
        "\n",
        "    if ARGS.warm < START_EPOCH and ARGS.over:\n",
        "        raise ValueError(\"warm < START_EPOCH\")\n",
        "\n",
        "    SUCCESS = torch.zeros(EPOCH, N_CLASSES, 2)\n",
        "    test_stats = {}\n",
        "    for epoch in range(START_EPOCH, EPOCH):\n",
        "        logger.log(' * Epoch %d: %s' % (epoch, LOGDIR))\n",
        "\n",
        "        adjust_learning_rate(optimizer, LR, epoch)\n",
        "\n",
        "        if epoch == ARGS.warm and ARGS.over:\n",
        "            if ARGS.smote:\n",
        "                logger.log(\"=============== Applying smote sampling ===============\")\n",
        "                smote_loader, _, _ = get_smote(DATASET, N_SAMPLES_PER_CLASS, BATCH_SIZE, transform_train, transform_test)\n",
        "                smote_loader_inf = inf_data_gen(smote_loader)\n",
        "            else:\n",
        "                logger.log(\"=============== Applying over sampling ===============\")\n",
        "                train_loader, _, _ = get_oversampled(DATASET, N_SAMPLES_PER_CLASS, BATCH_SIZE,\n",
        "                                                     transform_train, transform_test)\n",
        "\n",
        "        ## For Cost-Sensitive Learning ##\n",
        "\n",
        "        if ARGS.cost and epoch >= ARGS.warm:\n",
        "            beta = ARGS.eff_beta\n",
        "            if beta < 1:\n",
        "                effective_num = 1.0 - np.power(beta, N_SAMPLES_PER_CLASS)\n",
        "                per_cls_weights = (1.0 - beta) / np.array(effective_num)\n",
        "            else:\n",
        "                per_cls_weights = 1 / np.array(N_SAMPLES_PER_CLASS)\n",
        "            per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(N_SAMPLES_PER_CLASS)\n",
        "            per_cls_weights = torch.FloatTensor(per_cls_weights).to(device)\n",
        "        else:\n",
        "            per_cls_weights = torch.ones(N_CLASSES).to(device)\n",
        "\n",
        "        ## Choos a loss function ##\n",
        "\n",
        "        if ARGS.loss_type == 'CE':\n",
        "            criterion = nn.CrossEntropyLoss(weight=per_cls_weights, reduction='none').to(device)\n",
        "        elif ARGS.loss_type == 'Focal':\n",
        "            criterion = FocalLoss(weight=per_cls_weights, gamma=ARGS.focal_gamma, reduction='none').to(device)\n",
        "        elif ARGS.loss_type == 'LDAM':\n",
        "            criterion = LDAMLoss(cls_num_list=N_SAMPLES_PER_CLASS, max_m=0.5, s=30, weight=per_cls_weights,\n",
        "                                 reduction='none').to(device)\n",
        "        else:\n",
        "            raise ValueError(\"Wrong Loss Type\")\n",
        "\n",
        "        ## Training ( ARGS.warm is used for deferred re-balancing ) ##\n",
        "\n",
        "        if epoch >= ARGS.warm and ARGS.gen:\n",
        "            train_stats = train_gen_epoch(net, net_seed, criterion, optimizer, train_loader)\n",
        "            SUCCESS[epoch, :, :] = train_stats['t_success'].float()\n",
        "            logger.log(SUCCESS[epoch, -10:, :])\n",
        "            np.save(LOGDIR + '/success.npy', SUCCESS.cpu().numpy())\n",
        "        else:\n",
        "            train_loss, train_acc = train_epoch(net, criterion, optimizer, train_loader, logger)\n",
        "            train_stats = {'train_loss': train_loss, 'train_acc': train_acc}\n",
        "            if epoch == 159:\n",
        "                save_checkpoint(train_acc, net, optimizer, epoch, True)\n",
        "\n",
        "        ## Evaluation ##\n",
        "\n",
        "        val_eval = evaluate(net, val_loader, logger=logger)\n",
        "        val_acc = val_eval['acc']\n",
        "        if val_acc >= BEST_VAL:\n",
        "            BEST_VAL = val_acc\n",
        "\n",
        "            test_stats = evaluate(net, test_loader, logger=logger)\n",
        "            TEST_ACC = test_stats['acc']\n",
        "            TEST_ACC_CLASS = test_stats['class_acc']\n",
        "\n",
        "            save_checkpoint(TEST_ACC, net, optimizer, epoch)\n",
        "            logger.log(\"========== Class-wise test performance ( avg : {} ) ==========\".format(TEST_ACC_CLASS.mean()))\n",
        "            np.save(LOGDIR + '/classwise_acc.npy', TEST_ACC_CLASS.cpu())\n",
        "\n",
        "        def _convert_scala(x):\n",
        "            if hasattr(x, 'item'):\n",
        "                x = x.item()\n",
        "            return x\n",
        "\n",
        "        log_tr = ['train_loss', 'gen_loss', 'train_acc', 'gen_acc', 'p_g_orig', 'p_g_targ']\n",
        "        log_te = ['loss', 'major_acc', 'neutral_acc', 'minor_acc', 'acc', 'f1_score']\n",
        "\n",
        "        log_vector = [epoch] + [train_stats.get(k, 0) for k in log_tr] + [test_stats.get(k, 0) for k in log_te]\n",
        "        log_vector = list(map(_convert_scala, log_vector))\n",
        "\n",
        "        with open(LOG_CSV, 'a') as f:\n",
        "            logwriter = csv.writer(f, delimiter=',')\n",
        "            logwriter.writerow(log_vector)\n",
        "\n",
        "    logger.log(' * %s' % LOGDIR)\n",
        "    logger.log(\"Best Accuracy : {}\".format(TEST_ACC))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}